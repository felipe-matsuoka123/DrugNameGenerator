{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_update(model, learning_rate):\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param.grad = None\n",
    "        for param in model.parameters():\n",
    "            param.data -= learning_rate * param.grad\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('/Users/felipeakiomatsuoka/Desktop/NLP/DrugNames.txt', 'r').read().split('\\n')\n",
    "names = [re.sub(r'[^a-zA-Z0-9 ]', '', name) for name in names]\n",
    "names = [name.lower() for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 37\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {ch:i + 1 for i, ch in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:ch for ch, i in stoi.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([882771, 4]), dtype: torch.int64\n",
      "Y shape: torch.Size([882771]), dtype: torch.int64\n",
      "tensor([0, 0, 0, 0])\n",
      "tensor(27)\n"
     ]
    }
   ],
   "source": [
    "block_size = 4\n",
    "batch_size = 32\n",
    "\n",
    "X, Y = [], []\n",
    "\n",
    "for name in names:\n",
    "    context = [0] * block_size\n",
    "    for ch in name + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y) \n",
    "\n",
    "print(f\"X shape: {X.shape}, dtype: {X.dtype}\")\n",
    "print(f\"Y shape: {Y.shape}, dtype: {Y.dtype}\")\n",
    "\n",
    "print(X[0])\n",
    "print(Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is: torch.Size([706216, 4])\n",
      "X_test shape is: torch.Size([176555, 4])\n",
      "Y_train shape is: torch.Size([706216])\n",
      "Y_test shape is: torch.Size([176555])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)    \n",
    "print(f\"X_train shape is: {X_train.shape}\")\n",
    "print(f\"X_test shape is: {X_test.shape}\")\n",
    "print(f\"Y_train shape is: {Y_train.shape}\")\n",
    "print(f\"Y_test shape is: {Y_test.shape}\")\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train,Y_train)\n",
    "test_dataset = TensorDataset(X_test,Y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, block_size,embedding_dim, n_hidden):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(block_size * embedding_dim, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            #nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            #nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            #nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            #nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden), nn.Tanh(),\n",
    "            nn.Linear(n_hidden, vocab_size, bias=False), nn.BatchNorm1d(vocab_size)\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.layers[-1].weight.data *= 0.1\n",
    "            for layer in self.layers[:-1]:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    layer.weight.data *= 5/3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (emb): Embedding(38, 10)\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=40, out_features=200, bias=False)\n",
      "    (1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Tanh()\n",
      "    (3): Linear(in_features=200, out_features=38, bias=False)\n",
      "    (4): BatchNorm1d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(len(stoi), block_size, 10, 200)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 16456\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of parameters:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 3.6664\n",
      "Epoch: 10000, Loss: 1.2775\n",
      "Epoch: 20000, Loss: 1.6483\n",
      "Epoch: 30000, Loss: 1.5610\n",
      "Epoch: 40000, Loss: 1.2053\n",
      "Epoch: 50000, Loss: 1.0707\n",
      "Epoch: 60000, Loss: 0.7556\n",
      "Epoch: 70000, Loss: 1.5316\n",
      "Epoch: 80000, Loss: 1.2795\n",
      "Epoch: 90000, Loss: 1.2239\n",
      "Epoch: 100000, Loss: 1.1112\n",
      "Epoch: 110000, Loss: 0.8924\n",
      "Epoch: 120000, Loss: 1.0001\n",
      "Epoch: 130000, Loss: 1.3853\n",
      "Epoch: 140000, Loss: 1.0483\n",
      "Epoch: 150000, Loss: 1.0810\n",
      "Epoch: 160000, Loss: 1.2999\n",
      "Epoch: 170000, Loss: 1.0369\n",
      "Epoch: 180000, Loss: 1.4115\n",
      "Epoch: 190000, Loss: 1.0116\n"
     ]
    }
   ],
   "source": [
    "epochs = 200000\n",
    "lr = 0.1\n",
    "e = 1e-8\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ix = torch.randint(0, len(X_train), (batch_size,))\n",
    "    Xb, Yb = X_train[ix], Y_train[ix]\n",
    "    Y_pred = model(Xb)\n",
    "    loss = F.cross_entropy(Y_pred, Yb)\n",
    "    for p in model.parameters():\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    lr = 0.1 if epoch < 100000 else 0.01 \n",
    "    for p in model.parameters():\n",
    "        p.data -= lr * p.grad\n",
    "    if epoch % 10000 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.0221842527389526\n",
      "test loss: 1.0418405532836914\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (X_train, Y_train),\n",
    "        'test': (X_test, Y_test)\n",
    "    }[split]\n",
    "    logits = model(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f\"{split} loss: {loss.item()}\")\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, block_size, itos, num_of_words, temperature=1.0):\n",
    "    for i in range(num_of_words):\n",
    "        context = [0] * block_size  # Initialize with zeros\n",
    "        out = []\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                x = torch.tensor([context], dtype=torch.long)\n",
    "                logits = model(x)\n",
    "                probs = F.softmax(logits / temperature, dim=-1).squeeze()\n",
    "                next_char_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "                context = context[1:] + [next_char_idx]  # Update context\n",
    "                out.append(next_char_idx)\n",
    "                if next_char_idx == 0:  # Assuming '\\0' is the end token\n",
    "                    break\n",
    "        print(''.join([itos[i] for i in out]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norphin.\n",
      "prrostigminix.\n",
      "cfimopssacandroxyzos.\n",
      "prednic acid.\n",
      "sulfate and hydrochloride.\n",
      "cefazole maleate.\n",
      "carbonate.\n",
      "hydrochloride.\n",
      "valprazole.\n",
      "methorphenavrin sodex.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor([[0] * block_size], dtype=torch.long)\n",
    "model.eval()\n",
    "generate_text(model, block_size, itos, num_of_words=10, temperature=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
